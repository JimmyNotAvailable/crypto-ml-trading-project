#!/usr/bin/env python3
"""
üéØ MODEL PERFORMANCE ANALYZER & ALGORITHM SELECTOR
==================================================

H·ªá th·ªëng t·ª± ƒë·ªông ph√¢n t√≠ch hi·ªáu su·∫•t v√† ch·ªçn thu·∫≠t to√°n t·ªët nh·∫•t:
- üìä Ph√¢n t√≠ch metrics t·ª´ training jobs v√† model registry
- üèÜ X·∫øp h·∫°ng thu·∫≠t to√°n theo hi·ªáu su·∫•t
- üéØ T·ª± ƒë·ªông ch·ªçn thu·∫≠t to√°n t·ªët nh·∫•t cho t·ª´ng task
- üìà Cung c·∫•p khuy·∫øn ngh·ªã d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import os
import sys
from pathlib import Path

# Add project root to path
project_root = str(Path(__file__).parent.parent.absolute())
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'app'))

class ModelPerformanceAnalyzer:
    """
    üîç Ph√¢n t√≠ch hi·ªáu su·∫•t c√°c thu·∫≠t to√°n ML
    """
    
    def __init__(self, training_jobs_path: Optional[str] = None, model_registry_path: Optional[str] = None):
        # T√¨m project root t·ª´ v·ªã tr√≠ hi·ªán t·∫°i
        current_path = Path(__file__).absolute()
        self.project_root = current_path.parent.parent.parent  # t·ª´ app/ml/ l√™n project root
        
        # Paths
        self.training_jobs_path = training_jobs_path or self.project_root / "training" / "training_jobs.json"
        self.model_registry_path = model_registry_path or self.project_root / "models" / "model_registry.json"
        
        # Load data
        self.training_jobs = self._load_training_jobs()
        self.model_registry = self._load_model_registry()
        
        # Performance metrics weights for scoring
        self.metric_weights = {
            'regression': {
                'r2': 0.5,           # R¬≤ score (higher = better)
                'mae': -0.3,         # Mean Absolute Error (lower = better)
                'rmse': -0.2         # Root Mean Square Error (lower = better)
            },
            'classification': {
                'accuracy': 0.6,     # Accuracy (higher = better)
                'precision': 0.2,    # Precision (higher = better)
                'recall': 0.2        # Recall (higher = better)
            },
            'clustering': {
                'silhouette_score': 0.7,    # Silhouette score (higher = better)
                'calinski_harabasz': 0.3    # Calinski-Harabasz index (higher = better)
            }
        }
    
    def _load_training_jobs(self) -> Dict:
        """T·∫£i d·ªØ li·ªáu training jobs"""
        try:
            with open(self.training_jobs_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {self.training_jobs_path}")
            return {"jobs": {}}
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i training jobs: {e}")
            return {"jobs": {}}
    
    def _load_model_registry(self) -> Dict:
        """T·∫£i d·ªØ li·ªáu model registry"""
        try:
            with open(self.model_registry_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {self.model_registry_path}")
            return {"models": {}}
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i model registry: {e}")
            return {"models": {}}
    
    def analyze_algorithm_performance(self) -> pd.DataFrame:
        """
        üìä Ph√¢n t√≠ch hi·ªáu su·∫•t c√°c thu·∫≠t to√°n
        
        Returns:
            DataFrame ch·ª©a th·ªëng k√™ hi·ªáu su·∫•t t·ª´ng thu·∫≠t to√°n
        """
        print("üîç Ph√¢n t√≠ch hi·ªáu su·∫•t c√°c thu·∫≠t to√°n...")
        
        algorithm_stats = {}
        
        # Ph√¢n t√≠ch t·ª´ training jobs
        for job_id, job_data in self.training_jobs.get('jobs', {}).items():
            if job_data.get('status') != 'completed':
                continue
                
            model_type = job_data.get('model_type', 'unknown')
            target_type = job_data.get('target_type', 'unknown')
            metrics = job_data.get('metrics', {})
            
            # T·∫°o key cho thu·∫≠t to√°n + target
            algo_key = f"{model_type}_{target_type}"
            
            if algo_key not in algorithm_stats:
                algorithm_stats[algo_key] = {
                    'algorithm': model_type,
                    'target_type': target_type,
                    'jobs_count': 0,
                    'avg_r2': [],
                    'avg_mae': [],
                    'avg_rmse': [],
                    'avg_accuracy': [],
                    'latest_job': job_data.get('created_at'),
                    'best_r2': 0,
                    'best_mae': float('inf'),
                    'performance_scores': []
                }
            
            # C·∫≠p nh·∫≠t th·ªëng k√™
            stats = algorithm_stats[algo_key]
            stats['jobs_count'] += 1
            
            # Metrics t·ª´ test set (quan tr·ªçng nh·∫•t)
            test_metrics = metrics.get('test', {})
            if test_metrics:
                r2 = test_metrics.get('r2', 0)
                mae = test_metrics.get('mae', 0)
                rmse = test_metrics.get('rmse', 0)
                accuracy = test_metrics.get('accuracy', 0)
                
                stats['avg_r2'].append(r2)
                stats['avg_mae'].append(mae)
                stats['avg_rmse'].append(rmse)
                if accuracy > 0:
                    stats['avg_accuracy'].append(accuracy)
                
                # C·∫≠p nh·∫≠t best scores
                if r2 > stats['best_r2']:
                    stats['best_r2'] = r2
                if mae > 0 and mae < stats['best_mae']:
                    stats['best_mae'] = mae
        
        # T√≠nh to√°n trung b√¨nh v√† t·∫°o DataFrame
        results = []
        for algo_key, stats in algorithm_stats.items():
            if stats['jobs_count'] == 0:
                continue
                
            result = {
                'algorithm': stats['algorithm'],
                'target_type': stats['target_type'],
                'jobs_count': stats['jobs_count'],
                'avg_r2': np.mean(stats['avg_r2']) if stats['avg_r2'] else 0,
                'avg_mae': np.mean(stats['avg_mae']) if stats['avg_mae'] else 0,
                'avg_rmse': np.mean(stats['avg_rmse']) if stats['avg_rmse'] else 0,
                'avg_accuracy': np.mean(stats['avg_accuracy']) if stats['avg_accuracy'] else 0,
                'best_r2': stats['best_r2'],
                'best_mae': stats['best_mae'] if stats['best_mae'] != float('inf') else 0,
                'latest_job': stats['latest_job']
            }
            
            # T√≠nh performance score
            result['performance_score'] = self._calculate_performance_score(result)
            results.append(result)
        
        # T·∫°o DataFrame v√† s·∫Øp x·∫øp theo performance score
        df = pd.DataFrame(results)
        if not df.empty:
            df = df.sort_values('performance_score', ascending=False).reset_index(drop=True)
            df['rank'] = range(1, len(df) + 1)
        
        return df
    
    def _calculate_performance_score(self, metrics: Dict) -> float:
        """
        üèÜ T√≠nh ƒëi·ªÉm hi·ªáu su·∫•t t·ªïng h·ª£p
        
        Args:
            metrics: Dictionary ch·ª©a c√°c metrics
            
        Returns:
            Performance score (0-100, c√†ng cao c√†ng t·ªët)
        """
        score = 0.0
        
        # Normalize R¬≤ score (0-100)
        r2_score = max(0, min(100, metrics.get('avg_r2', 0) * 100))
        score += r2_score * 0.5
        
        # Normalize MAE (convert to 0-100 scale, lower is better)
        mae = metrics.get('avg_mae', 0)
        if mae > 0:
            # For crypto prices, MAE around 20-50 is good, >100 is poor
            mae_score = max(0, 100 - (mae / 100) * 100)
            score += mae_score * 0.3
        
        # Accuracy for classification
        accuracy = metrics.get('avg_accuracy', 0)
        if accuracy > 0:
            score += accuracy * 100 * 0.6
        
        # Bonus for stability (multiple successful jobs)
        jobs_count = metrics.get('jobs_count', 1)
        stability_bonus = min(10, jobs_count * 2)  # Max 10 points
        score += stability_bonus
        
        return round(score, 2)
    
    def get_algorithm_ranking(self, target_type: Optional[str] = None) -> pd.DataFrame:
        """
        üèÖ L·∫•y x·∫øp h·∫°ng thu·∫≠t to√°n
        
        Args:
            target_type: Lo·∫°i target c·∫ßn l·ªçc ('price', 'price_change', None cho t·∫•t c·∫£)
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c x·∫øp h·∫°ng
        """
        df = self.analyze_algorithm_performance()
        
        if df.empty:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·∫øp h·∫°ng")
            return pd.DataFrame()
        
        if target_type:
            df = df[df['target_type'] == target_type].reset_index(drop=True)
            df['rank'] = range(1, len(df) + 1)
        
        return df
    
    def recommend_best_algorithm(self, target_type: str, task_type: str = 'regression') -> Dict:
        """
        üéØ Khuy·∫øn ngh·ªã thu·∫≠t to√°n t·ªët nh·∫•t
        
        Args:
            target_type: Lo·∫°i target ('price', 'price_change', etc.)
            task_type: Lo·∫°i task ('regression', 'classification', 'clustering')
            
        Returns:
            Dictionary ch·ª©a th√¥ng tin thu·∫≠t to√°n ƒë∆∞·ª£c khuy·∫øn ngh·ªã
        """
        ranking = self.get_algorithm_ranking(target_type)
        
        if ranking.empty:
            return {
                'algorithm': 'linear_regression',  # Default fallback
                'confidence': 'low',
                'reason': 'Kh√¥ng c√≥ d·ªØ li·ªáu training ƒë·ªÉ ƒë√°nh gi√°',
                'performance_score': 0
            }
        
        best_algo = ranking.iloc[0]
        
        # ƒê√°nh gi√° confidence d·ª±a tr√™n performance score
        score = best_algo['performance_score']
        if score >= 80:
            confidence = 'very_high'
        elif score >= 60:
            confidence = 'high'
        elif score >= 40:
            confidence = 'medium'
        else:
            confidence = 'low'
        
        return {
            'algorithm': best_algo['algorithm'],
            'target_type': target_type,
            'performance_score': score,
            'confidence': confidence,
            'jobs_count': best_algo['jobs_count'],
            'avg_r2': best_algo['avg_r2'],
            'avg_mae': best_algo['avg_mae'],
            'rank': 1,
            'reason': f"Thu·∫≠t to√°n t·ªët nh·∫•t v·ªõi ƒëi·ªÉm s·ªë {score:.1f}/100 t·ª´ {best_algo['jobs_count']} l·∫ßn training"
        }

class AlgorithmSelector:
    """
    üéØ B·ªô ch·ªçn thu·∫≠t to√°n t·ª± ƒë·ªông
    """
    
    def __init__(self):
        self.analyzer = ModelPerformanceAnalyzer()
    
    def select_best_algorithm_for_task(self, target_type: str, 
                                     min_confidence: str = 'medium') -> Dict:
        """
        üèÜ Ch·ªçn thu·∫≠t to√°n t·ªët nh·∫•t cho task c·ª• th·ªÉ
        
        Args:
            target_type: Lo·∫°i target c·∫ßn d·ª± ƒëo√°n
            min_confidence: Confidence t·ªëi thi·ªÉu ('low', 'medium', 'high', 'very_high')
            
        Returns:
            Dictionary ch·ª©a th√¥ng tin thu·∫≠t to√°n ƒë∆∞·ª£c ch·ªçn
        """
        recommendation = self.analyzer.recommend_best_algorithm(target_type)
        
        confidence_levels = {'low': 1, 'medium': 2, 'high': 3, 'very_high': 4}
        min_level = confidence_levels.get(min_confidence, 2)
        current_level = confidence_levels.get(recommendation['confidence'], 1)
        
        if current_level >= min_level:
            return {
                'selected': True,
                'algorithm': recommendation['algorithm'],
                'confidence': recommendation['confidence'],
                'performance_score': recommendation['performance_score'],
                'reason': recommendation['reason']
            }
        else:
            return {
                'selected': False,
                'algorithm': 'linear_regression',  # Safe fallback
                'confidence': 'fallback',
                'performance_score': 0,
                'reason': f"Confidence {recommendation['confidence']} < y√™u c·∫ßu {min_confidence}, s·ª≠ d·ª•ng fallback"
            }
    
    def get_performance_report(self) -> str:
        """
        üìä T·∫°o b√°o c√°o hi·ªáu su·∫•t t·ªïng quan
        
        Returns:
            String ch·ª©a b√°o c√°o ƒë·ªãnh d·∫°ng
        """
        ranking = self.analyzer.get_algorithm_ranking()
        
        if ranking.empty:
            return "‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o b√°o c√°o hi·ªáu su·∫•t"
        
        report = "üèÜ B√ÅO C√ÅO HI·ªÜU SU·∫§T THU·∫¨T TO√ÅN\n"
        report += "=" * 50 + "\n\n"
        
        # Top performers
        report += "ü•á TOP 5 THU·∫¨T TO√ÅN T·ªêT NH·∫§T:\n"
        report += "-" * 30 + "\n"
        
        for idx, (i, row) in enumerate(ranking.head(5).iterrows()):
            medals = ["ü•á", "ü•à", "ü•â", "4Ô∏è‚É£", "5Ô∏è‚É£"]
            medal = medals[idx] if idx < len(medals) else f"{idx+1}Ô∏è‚É£"
            report += f"{medal} {row['algorithm']} ({row['target_type']})\n"
            report += f"   üìä ƒêi·ªÉm s·ªë: {row['performance_score']:.1f}/100\n"
            report += f"   üìà R¬≤: {row['avg_r2']:.4f}\n"
            report += f"   üìä MAE: {row['avg_mae']:.2f}\n"
            report += f"   üîÑ S·ªë l·∫ßn training: {row['jobs_count']}\n\n"
        
        # Performance by target type
        report += "üìä HI·ªÜU SU·∫§T THEO LO·∫†I TARGET:\n"
        report += "-" * 30 + "\n"
        
        for target_type in ranking['target_type'].unique():
            target_data = ranking[ranking['target_type'] == target_type]
            best = target_data.iloc[0]
            report += f"üéØ {target_type.upper()}:\n"
            report += f"   ‚úÖ T·ªët nh·∫•t: {best['algorithm']} (ƒëi·ªÉm: {best['performance_score']:.1f})\n"
            report += f"   üìä S·ªë thu·∫≠t to√°n kh·∫£ d·ª•ng: {len(target_data)}\n\n"
        
        return report

def main():
    """Demo function"""
    print("üéØ DEMO ALGORITHM SELECTOR")
    print("=" * 50)
    
    # Kh·ªüi t·∫°o analyzer
    selector = AlgorithmSelector()
    
    # Test cho price prediction
    print("\nüîç PH√ÇN T√çCH CHO D·ª∞ ƒêO√ÅN GI√Å:")
    price_selection = selector.select_best_algorithm_for_task('price')
    print(f"üèÜ Thu·∫≠t to√°n ƒë∆∞·ª£c ch·ªçn: {price_selection['algorithm']}")
    print(f"üìä ƒêi·ªÉm s·ªë: {price_selection['performance_score']:.1f}")
    print(f"üéØ Confidence: {price_selection['confidence']}")
    print(f"üí° L√Ω do: {price_selection['reason']}")
    
    # Test cho price change prediction  
    print("\nüîç PH√ÇN T√çCH CHO D·ª∞ ƒêO√ÅN THAY ƒê·ªîI GI√Å:")
    change_selection = selector.select_best_algorithm_for_task('price_change')
    print(f"üèÜ Thu·∫≠t to√°n ƒë∆∞·ª£c ch·ªçn: {change_selection['algorithm']}")
    print(f"üìä ƒêi·ªÉm s·ªë: {change_selection['performance_score']:.1f}")
    print(f"üéØ Confidence: {change_selection['confidence']}")
    print(f"üí° L√Ω do: {change_selection['reason']}")
    
    # B√°o c√°o t·ªïng quan
    print("\n" + selector.get_performance_report())

if __name__ == "__main__":
    main()