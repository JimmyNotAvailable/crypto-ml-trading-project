#!/usr/bin/env python3
"""
ğŸ¯ MODEL PERFORMANCE ANALYZER & ALGORITHM SELECTOR
==================================================

Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¢n tÃ­ch hiá»‡u suáº¥t vÃ  chá»n thuáº­t toÃ¡n tá»‘t nháº¥t:
- ğŸ“Š PhÃ¢n tÃ­ch metrics tá»« training jobs vÃ  model registry
- ğŸ† Xáº¿p háº¡ng thuáº­t toÃ¡n theo hiá»‡u suáº¥t
- ğŸ¯ Tá»± Ä‘á»™ng chá»n thuáº­t toÃ¡n tá»‘t nháº¥t cho tá»«ng task
- ğŸ“ˆ Cung cáº¥p khuyáº¿n nghá»‹ dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import os
import sys
from pathlib import Path

# Add project root to path
project_root = str(Path(__file__).parent.parent.absolute())
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'app'))

class ModelPerformanceAnalyzer:
    """
    ğŸ” PhÃ¢n tÃ­ch hiá»‡u suáº¥t cÃ¡c thuáº­t toÃ¡n ML
    """
    
    def __init__(self, training_jobs_path: str = None, model_registry_path: str = None):
        # TÃ¬m project root tá»« vá»‹ trÃ­ hiá»‡n táº¡i
        current_path = Path(__file__).absolute()
        self.project_root = current_path.parent.parent.parent  # tá»« app/ml/ lÃªn project root
        
        # Paths
        self.training_jobs_path = training_jobs_path or self.project_root / "training" / "training_jobs.json"
        self.model_registry_path = model_registry_path or self.project_root / "models" / "model_registry.json"
        
        # Load data
        self.training_jobs = self._load_training_jobs()
        self.model_registry = self._load_model_registry()
        
        # Performance metrics weights for scoring
        self.metric_weights = {
            'regression': {
                'r2': 0.5,           # RÂ² score (higher = better)
                'mae': -0.3,         # Mean Absolute Error (lower = better)
                'rmse': -0.2         # Root Mean Square Error (lower = better)
            },
            'classification': {
                'accuracy': 0.6,     # Accuracy (higher = better)
                'precision': 0.2,    # Precision (higher = better)
                'recall': 0.2        # Recall (higher = better)
            },
            'clustering': {
                'silhouette_score': 0.7,    # Silhouette score (higher = better)
                'calinski_harabasz': 0.3    # Calinski-Harabasz index (higher = better)
            }
        }
    
    def _load_training_jobs(self) -> Dict:
        """Táº£i dá»¯ liá»‡u training jobs"""
        try:
            with open(self.training_jobs_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file: {self.training_jobs_path}")
            return {"jobs": {}}
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i training jobs: {e}")
            return {"jobs": {}}
    
    def _load_model_registry(self) -> Dict:
        """Táº£i dá»¯ liá»‡u model registry"""
        try:
            with open(self.model_registry_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file: {self.model_registry_path}")
            return {"models": {}}
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i model registry: {e}")
            return {"models": {}}
    
    def analyze_algorithm_performance(self) -> pd.DataFrame:
        """
        ğŸ“Š PhÃ¢n tÃ­ch hiá»‡u suáº¥t cÃ¡c thuáº­t toÃ¡n
        
        Returns:
            DataFrame chá»©a thá»‘ng kÃª hiá»‡u suáº¥t tá»«ng thuáº­t toÃ¡n
        """
        print("ğŸ” PhÃ¢n tÃ­ch hiá»‡u suáº¥t cÃ¡c thuáº­t toÃ¡n...")
        
        algorithm_stats = {}
        
        # PhÃ¢n tÃ­ch tá»« training jobs
        for job_id, job_data in self.training_jobs.get('jobs', {}).items():
            if job_data.get('status') != 'completed':
                continue
                
            model_type = job_data.get('model_type', 'unknown')
            target_type = job_data.get('target_type', 'unknown')
            metrics = job_data.get('metrics', {})
            
            # Táº¡o key cho thuáº­t toÃ¡n + target
            algo_key = f"{model_type}_{target_type}"
            
            if algo_key not in algorithm_stats:
                algorithm_stats[algo_key] = {
                    'algorithm': model_type,
                    'target_type': target_type,
                    'jobs_count': 0,
                    'avg_r2': [],
                    'avg_mae': [],
                    'avg_rmse': [],
                    'avg_accuracy': [],
                    'latest_job': job_data.get('created_at'),
                    'best_r2': 0,
                    'best_mae': float('inf'),
                    'performance_scores': []
                }
            
            # Cáº­p nháº­t thá»‘ng kÃª
            stats = algorithm_stats[algo_key]
            stats['jobs_count'] += 1
            
            # Metrics tá»« test set (quan trá»ng nháº¥t)
            test_metrics = metrics.get('test', {})
            if test_metrics:
                r2 = test_metrics.get('r2', 0)
                mae = test_metrics.get('mae', 0)
                rmse = test_metrics.get('rmse', 0)
                accuracy = test_metrics.get('accuracy', 0)
                
                stats['avg_r2'].append(r2)
                stats['avg_mae'].append(mae)
                stats['avg_rmse'].append(rmse)
                if accuracy > 0:
                    stats['avg_accuracy'].append(accuracy)
                
                # Cáº­p nháº­t best scores
                if r2 > stats['best_r2']:
                    stats['best_r2'] = r2
                if mae > 0 and mae < stats['best_mae']:
                    stats['best_mae'] = mae
        
        # TÃ­nh toÃ¡n trung bÃ¬nh vÃ  táº¡o DataFrame
        results = []
        for algo_key, stats in algorithm_stats.items():
            if stats['jobs_count'] == 0:
                continue
                
            result = {
                'algorithm': stats['algorithm'],
                'target_type': stats['target_type'],
                'jobs_count': stats['jobs_count'],
                'avg_r2': np.mean(stats['avg_r2']) if stats['avg_r2'] else 0,
                'avg_mae': np.mean(stats['avg_mae']) if stats['avg_mae'] else 0,
                'avg_rmse': np.mean(stats['avg_rmse']) if stats['avg_rmse'] else 0,
                'avg_accuracy': np.mean(stats['avg_accuracy']) if stats['avg_accuracy'] else 0,
                'best_r2': stats['best_r2'],
                'best_mae': stats['best_mae'] if stats['best_mae'] != float('inf') else 0,
                'latest_job': stats['latest_job']
            }
            
            # TÃ­nh performance score
            result['performance_score'] = self._calculate_performance_score(result)
            results.append(result)
        
        # Táº¡o DataFrame vÃ  sáº¯p xáº¿p theo performance score
        df = pd.DataFrame(results)
        if not df.empty:
            df = df.sort_values('performance_score', ascending=False).reset_index(drop=True)
            df['rank'] = range(1, len(df) + 1)
        
        return df
    
    def _calculate_performance_score(self, metrics: Dict) -> float:
        """
        ğŸ† TÃ­nh Ä‘iá»ƒm hiá»‡u suáº¥t tá»•ng há»£p
        
        Args:
            metrics: Dictionary chá»©a cÃ¡c metrics
            
        Returns:
            Performance score (0-100, cÃ ng cao cÃ ng tá»‘t)
        """
        score = 0.0
        
        # Normalize RÂ² score (0-100)
        r2_score = max(0, min(100, metrics.get('avg_r2', 0) * 100))
        score += r2_score * 0.5
        
        # Normalize MAE (convert to 0-100 scale, lower is better)
        mae = metrics.get('avg_mae', 0)
        if mae > 0:
            # For crypto prices, MAE around 20-50 is good, >100 is poor
            mae_score = max(0, 100 - (mae / 100) * 100)
            score += mae_score * 0.3
        
        # Accuracy for classification
        accuracy = metrics.get('avg_accuracy', 0)
        if accuracy > 0:
            score += accuracy * 100 * 0.6
        
        # Bonus for stability (multiple successful jobs)
        jobs_count = metrics.get('jobs_count', 1)
        stability_bonus = min(10, jobs_count * 2)  # Max 10 points
        score += stability_bonus
        
        return round(score, 2)
    
    def get_algorithm_ranking(self, target_type: str = None) -> pd.DataFrame:
        """
        ğŸ… Láº¥y xáº¿p háº¡ng thuáº­t toÃ¡n
        
        Args:
            target_type: Loáº¡i target cáº§n lá»c ('price', 'price_change', None cho táº¥t cáº£)
            
        Returns:
            DataFrame Ä‘Ã£ Ä‘Æ°á»£c xáº¿p háº¡ng
        """
        df = self.analyze_algorithm_performance()
        
        if df.empty:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xáº¿p háº¡ng")
            return pd.DataFrame()
        
        if target_type:
            df = df[df['target_type'] == target_type].reset_index(drop=True)
            df['rank'] = range(1, len(df) + 1)
        
        return df
    
    def recommend_best_algorithm(self, target_type: str, task_type: str = 'regression') -> Dict:
        """
        ğŸ¯ Khuyáº¿n nghá»‹ thuáº­t toÃ¡n tá»‘t nháº¥t
        
        Args:
            target_type: Loáº¡i target ('price', 'price_change', etc.)
            task_type: Loáº¡i task ('regression', 'classification', 'clustering')
            
        Returns:
            Dictionary chá»©a thÃ´ng tin thuáº­t toÃ¡n Ä‘Æ°á»£c khuyáº¿n nghá»‹
        """
        ranking = self.get_algorithm_ranking(target_type)
        
        if ranking.empty:
            return {
                'algorithm': 'linear_regression',  # Default fallback
                'confidence': 'low',
                'reason': 'KhÃ´ng cÃ³ dá»¯ liá»‡u training Ä‘á»ƒ Ä‘Ã¡nh giÃ¡',
                'performance_score': 0
            }
        
        best_algo = ranking.iloc[0]
        
        # ÄÃ¡nh giÃ¡ confidence dá»±a trÃªn performance score
        score = best_algo['performance_score']
        if score >= 80:
            confidence = 'very_high'
        elif score >= 60:
            confidence = 'high'
        elif score >= 40:
            confidence = 'medium'
        else:
            confidence = 'low'
        
        return {
            'algorithm': best_algo['algorithm'],
            'target_type': target_type,
            'performance_score': score,
            'confidence': confidence,
            'jobs_count': best_algo['jobs_count'],
            'avg_r2': best_algo['avg_r2'],
            'avg_mae': best_algo['avg_mae'],
            'rank': 1,
            'reason': f"Thuáº­t toÃ¡n tá»‘t nháº¥t vá»›i Ä‘iá»ƒm sá»‘ {score:.1f}/100 tá»« {best_algo['jobs_count']} láº§n training"
        }

class AlgorithmSelector:
    """
    ğŸ¯ Bá»™ chá»n thuáº­t toÃ¡n tá»± Ä‘á»™ng
    """
    
    def __init__(self):
        self.analyzer = ModelPerformanceAnalyzer()
    
    def select_best_algorithm_for_task(self, target_type: str, 
                                     min_confidence: str = 'medium') -> Dict:
        """
        ğŸ† Chá»n thuáº­t toÃ¡n tá»‘t nháº¥t cho task cá»¥ thá»ƒ
        
        Args:
            target_type: Loáº¡i target cáº§n dá»± Ä‘oÃ¡n
            min_confidence: Confidence tá»‘i thiá»ƒu ('low', 'medium', 'high', 'very_high')
            
        Returns:
            Dictionary chá»©a thÃ´ng tin thuáº­t toÃ¡n Ä‘Æ°á»£c chá»n
        """
        recommendation = self.analyzer.recommend_best_algorithm(target_type)
        
        confidence_levels = {'low': 1, 'medium': 2, 'high': 3, 'very_high': 4}
        min_level = confidence_levels.get(min_confidence, 2)
        current_level = confidence_levels.get(recommendation['confidence'], 1)
        
        if current_level >= min_level:
            return {
                'selected': True,
                'algorithm': recommendation['algorithm'],
                'confidence': recommendation['confidence'],
                'performance_score': recommendation['performance_score'],
                'reason': recommendation['reason']
            }
        else:
            return {
                'selected': False,
                'algorithm': 'linear_regression',  # Safe fallback
                'confidence': 'fallback',
                'performance_score': 0,
                'reason': f"Confidence {recommendation['confidence']} < yÃªu cáº§u {min_confidence}, sá»­ dá»¥ng fallback"
            }
    
    def get_performance_report(self) -> str:
        """
        ğŸ“Š Táº¡o bÃ¡o cÃ¡o hiá»‡u suáº¥t tá»•ng quan
        
        Returns:
            String chá»©a bÃ¡o cÃ¡o Ä‘á»‹nh dáº¡ng
        """
        ranking = self.analyzer.get_algorithm_ranking()
        
        if ranking.empty:
            return "âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ táº¡o bÃ¡o cÃ¡o hiá»‡u suáº¥t"
        
        report = "ğŸ† BÃO CÃO HIá»†U SUáº¤T THUáº¬T TOÃN\n"
        report += "=" * 50 + "\n\n"
        
        # Top performers
        report += "ğŸ¥‡ TOP 5 THUáº¬T TOÃN Tá»T NHáº¤T:\n"
        report += "-" * 30 + "\n"
        
        for i, row in ranking.head(5).iterrows():
            medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"][i]
            report += f"{medal} {row['algorithm']} ({row['target_type']})\n"
            report += f"   ğŸ“Š Äiá»ƒm sá»‘: {row['performance_score']:.1f}/100\n"
            report += f"   ğŸ“ˆ RÂ²: {row['avg_r2']:.4f}\n"
            report += f"   ğŸ“Š MAE: {row['avg_mae']:.2f}\n"
            report += f"   ğŸ”„ Sá»‘ láº§n training: {row['jobs_count']}\n\n"
        
        # Performance by target type
        report += "ğŸ“Š HIá»†U SUáº¤T THEO LOáº I TARGET:\n"
        report += "-" * 30 + "\n"
        
        for target_type in ranking['target_type'].unique():
            target_data = ranking[ranking['target_type'] == target_type]
            best = target_data.iloc[0]
            report += f"ğŸ¯ {target_type.upper()}:\n"
            report += f"   âœ… Tá»‘t nháº¥t: {best['algorithm']} (Ä‘iá»ƒm: {best['performance_score']:.1f})\n"
            report += f"   ğŸ“Š Sá»‘ thuáº­t toÃ¡n kháº£ dá»¥ng: {len(target_data)}\n\n"
        
        return report

def main():
    """Demo function"""
    print("ğŸ¯ DEMO ALGORITHM SELECTOR")
    print("=" * 50)
    
    # Khá»Ÿi táº¡o analyzer
    selector = AlgorithmSelector()
    
    # Test cho price prediction
    print("\nğŸ” PHÃ‚N TÃCH CHO Dá»° ÄOÃN GIÃ:")
    price_selection = selector.select_best_algorithm_for_task('price')
    print(f"ğŸ† Thuáº­t toÃ¡n Ä‘Æ°á»£c chá»n: {price_selection['algorithm']}")
    print(f"ğŸ“Š Äiá»ƒm sá»‘: {price_selection['performance_score']:.1f}")
    print(f"ğŸ¯ Confidence: {price_selection['confidence']}")
    print(f"ğŸ’¡ LÃ½ do: {price_selection['reason']}")
    
    # Test cho price change prediction  
    print("\nğŸ” PHÃ‚N TÃCH CHO Dá»° ÄOÃN THAY Äá»”I GIÃ:")
    change_selection = selector.select_best_algorithm_for_task('price_change')
    print(f"ğŸ† Thuáº­t toÃ¡n Ä‘Æ°á»£c chá»n: {change_selection['algorithm']}")
    print(f"ğŸ“Š Äiá»ƒm sá»‘: {change_selection['performance_score']:.1f}")
    print(f"ğŸ¯ Confidence: {change_selection['confidence']}")
    print(f"ğŸ’¡ LÃ½ do: {change_selection['reason']}")
    
    # BÃ¡o cÃ¡o tá»•ng quan
    print("\n" + selector.get_performance_report())

if __name__ == "__main__":
    main()